=== Moving to Kubernetes on AWS at Scale
**Speaker: Daniel Nelson**

* tired of managing servers
* Tired of managing config mgmt
* Had a by-hand env and dev did not look like prod
* wanted
    * prod and dev use same image
    * easily add/remove machine
    * minimal maintenance
    * self-service
* OS: wanted minimal.
    * reduce temptation to ssh in and 'do stuff'
    * chose CoreOS
* Clusters:  Mesos, Fleet, Swarm, Amazon ECS, Kubernetes
    * chose Kubernetes, but ran against `kube-up.sh` limitations
* Tried CloudFormation. It gets very big (4000 lines) but it works
* Challenges
    * Needed Ingress LB. hacked to get around it for now.
* Migration
    * kafka is pipeline. Used mirroring between accounts once all producers were in new cluster/acct.
    * Worked with all app teams to get set up with kubernetes and docker deploys
* Wanted multi-region, still working on it
* single cluster across multiple AZs, hoping Kub will support better.
* RC are labeled to pin to a particular zone so all apps are multi-zone (since k8s). master can go down.
* nodes in ASG and autoregister
* assumes machines go down. Kub protects them from 2am pages
* Can save money by using Spot with spot fleet. Can over provision with pricing and still save money. Will watch eviction notice and then drain machine when that ability lands in Kube 1.2+
* ship with kafka because internet is not reliable.
* recommends Tinc for VPN
* Deployment - still working on
    * have a standard and track in metadata repo
    * metadata repo will generate the specs?
    * empower the project teams to update that metadata
    * automate.


